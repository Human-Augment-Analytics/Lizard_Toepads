{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763edb5c-115b-44a3-a736-5f7154a5e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa6f6c2b-9305-4576-8195-7037f5513089",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_path = \"../yolo/yolo_h5.pt\"\n",
    "test_image_id = 1075\n",
    "training_data_dir = \"../data/training_data\"\n",
    "test_data_dir = \"../data/test_data\"\n",
    "imgdir = \"../data/miami_fall_24_jpgs\"\n",
    "\n",
    "def get_img(img_id):\n",
    "    return cv2.imread(f\"../data/miami_fall_24_jpgs/{img_id}.jpg\")\n",
    "\n",
    "def get_tps_coords(img_id, img):\n",
    "    cls = [\"finger\", \"toe\"]\n",
    "    ret = {}\n",
    "    h, w = img.shape[:2]\n",
    "    for c in cls:\n",
    "        fp = f\"../data/tps_files/{img_id}_{c}.TPS\"\n",
    "        coordinates = []\n",
    "        skip = 2\n",
    "        with open(fp, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or \"=\" in line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:\n",
    "                    if skip > 0:\n",
    "                        skip -= 1\n",
    "                        continue\n",
    "                    try:\n",
    "                        x , y = map(float, parts)\n",
    "                        \n",
    "                        coordinates.append((x, h - 1 - y))\n",
    "                        #print((x, y))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        ret[c] = coordinates\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a70c030-b841-43e3-851f-11a98914cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_toe_boxes(r, image, g_coords, show=False, output_name=\"output\"):\n",
    "    target_classes = [2, 3] #[\"bot_finger\", \"bot_toe\"]\n",
    "    test_classes = [0, 1] #[\"up_finger\", \"up_toe\"]\n",
    "    classmap = {2: \"finger\", 3: \"toe\"}\n",
    "    crops = []  # store cropped images\n",
    "    coords_list = []  # store corresponding coordinates (for later heatmaps)\n",
    "    tps = []\n",
    "    result = r[0]  # first image\n",
    "    boxes = result.boxes\n",
    "    \n",
    "    # Convert to numpy arrays for convenience\n",
    "    xyxy = boxes.xyxy.cpu().numpy()   # shape (N, 4)\n",
    "    cls_ids = boxes.cls.cpu().numpy() # shape (N,)\n",
    "    conf = boxes.conf.cpu().numpy()   # optional if you want confidence filtering\n",
    "    \n",
    "    # Loop and filter\n",
    "    for (x1, y1, x2, y2), cls_id in zip(xyxy, cls_ids):\n",
    "        if int(cls_id) in target_classes:\n",
    "            # Crop the image\n",
    "            x1i, y1i, x2i, y2i = map(int, [x1, y1, x2, y2])\n",
    "            crop = image[y1i:y2i, x1i:x2i].copy()  # copy to avoid referencing original image\n",
    "            coords_list.append([x1i, y1i, x2i, y2i])  # store original coordinates\n",
    "            \n",
    "            l_coords = []\n",
    "            valid = True\n",
    "            for (x, y) in g_coords[classmap[int(cls_id)]]:\n",
    "                x_local = x - x1\n",
    "                y_local = y - y1\n",
    "                # Check valid\n",
    "                if not (0 <= x_local < (x2i - x1i) and 0 <= y_local < (y2i - y1i)):\n",
    "                    valid = False\n",
    "                    break\n",
    "\n",
    "                l_coords.append((x_local, y_local))\n",
    "\n",
    "            if not valid:\n",
    "                continue\n",
    "            \n",
    "            crops.append(crop)\n",
    "            tps.append(l_coords)\n",
    "\n",
    "            if show:\n",
    "                copy = crop.copy()\n",
    "                for (lx, ly) in l_coords:\n",
    "                    cv2.circle(copy, (int(round(lx)), int(round(ly))), 5, (0, 0, 255), -1)\n",
    "                cv2.imshow(f\"Crop Class {int(cls_id)}\", copy)\n",
    "                cv2.waitKey(0)   # waits for a key press\n",
    "                cv2.destroyWindow(f\"Crop Class {int(cls_id)}\")\n",
    "        elif int(cls_id) in test_classes:\n",
    "            x1i, y1i, x2i, y2i = map(int, [x1, y1, x2, y2])\n",
    "            crop = image[y1i:y2i, x1i:x2i].copy()\n",
    "            outpath = f\"{test_data_dir}/{output_name}_{classmap[int(cls_id)+2]}.jpg\"\n",
    "            #print(outpath)\n",
    "            cv2.imwrite(outpath, crop)\n",
    "            \n",
    "    return crops, coords_list, tps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6905fbf-2c0a-4609-96bb-b811d4a7f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axu16\\AppData\\Local\\Temp\\ipykernel_32472\\2429888909.py:4: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(\n"
     ]
    }
   ],
   "source": [
    "IMAGENORMALIZE = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=224),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=224,\n",
    "                min_width=224,\n",
    "                border_mode=0,\n",
    "                value=0,\n",
    "            ),\n",
    "            A.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        keypoint_params=A.KeypointParams(\n",
    "            format=\"xy\",\n",
    "            remove_invisible=False\n",
    "        )\n",
    "    )\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess_image(crop, tps):\n",
    "    aug = IMAGENORMALIZE(image=crop, keypoints=tps)\n",
    "    return aug[\"image\"], aug[\"keypoints\"]\n",
    "\n",
    "def produce_training(r, img, tps_coords, name=\"sample\"):\n",
    "    crops, box_coords, local_tps_coords = crop_toe_boxes(r, img, tps_coords, False)\n",
    "    a_rescaled = []\n",
    "    a_coords = []\n",
    "    for i in range(len(crops)):\n",
    "        if len(local_tps_coords[i]) != 9:\n",
    "            continue\n",
    "        rescaled, r_coords = preprocess_image(crops[i], local_tps_coords[i])\n",
    "        data = {\n",
    "            \"image\": rescaled,\n",
    "            \"keypoints\": torch.tensor(r_coords, dtype=torch.float32)\n",
    "        }\n",
    "        torch.save(data, f\"{training_data_dir}/vit/{name}_{i}.pt\")\n",
    "        a_rescaled.append(rescaled)\n",
    "        a_coords.append(r_coords)\n",
    "\n",
    "def inspect_data(img_tensor, tps):\n",
    "    img = img_tensor.permute(1,2,0).cpu().numpy()\n",
    "    img = img * IMAGENET_STD + IMAGENET_MEAN   # denormalize\n",
    "    img = (img * 255).clip(0,255).astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    for i, (x, y) in enumerate(tps):\n",
    "        x, y = int(x), int(y)\n",
    "        cv2.circle(img, (x,y), 4, (0,0,255), -1)\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            str(i),\n",
    "            (x+5, y-5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0,255,0),\n",
    "            1,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "    cv2.imshow(\"Debug\", img)\n",
    "\n",
    "    #print(\"Press any key for next image, or 'q' to quit\")\n",
    "    key = cv2.waitKey(0)\n",
    "\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        return False  # stop loop\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c70e3294-433b-4ede-9263-37ff7aab26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images():\n",
    "    model = YOLO(yolo_path)\n",
    "    dir_path = Path(imgdir)\n",
    "    count = 0\n",
    "    for file in dir_path.iterdir():\n",
    "        print(f\"Processing file {count}\", end=\"\\r\", flush=True)\n",
    "        try:\n",
    "            if \".jpg\" in file.name:\n",
    "                imgid = file.name.replace(\".jpg\", \"\")\n",
    "                if int(imgid) > 1000:\n",
    "                    process_image(imgid, model)\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            print()\n",
    "            print(f\"Failed to process file {file}: {e}\")\n",
    "            #continue\n",
    "            break\n",
    "\n",
    "def load_pts():\n",
    "    dir_path = Path(f\"{training_data_dir}/vit\")\n",
    "    pt_files = sorted(dir_path.glob(\"*.pt\"))\n",
    "    for pt_file in pt_files:\n",
    "        data = torch.load(pt_file)\n",
    "        image_tensor = data[\"image\"]\n",
    "        keypoints = data[\"keypoints\"]\n",
    "        cont = inspect_data(image_tensor, keypoints)\n",
    "        if not cont:\n",
    "            break\n",
    "\n",
    "def process_image(imgid, model):\n",
    "    img = get_img(imgid)\n",
    "    tps = get_tps_coords(imgid, img)\n",
    "    r = model(img, verbose=False)\n",
    "    produce_training(r, img, tps, imgid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aeec0c3-af67-4404-b51d-6e97f2299fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 849\n",
      "Failed to process file ..\\data\\miami_fall_24_jpgs\\2.99.jpg: invalid literal for int() with base 10: '2.99'\n"
     ]
    }
   ],
   "source": [
    "process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7d7000d-9f92-40e3-975f-45055cce1e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 1623\n",
      "Image min/max/mean (examples): [    -2.1179     -2.1179     -1.1932] [    0.91451       1.786      2.6226] -0.5137831589248799\n",
      "Kp min/max examples: [    0.27038      18.723      39.233] [     185.77      208.28      223.97]\n",
      "Keypoint tensor sizes (numel): unique: {18}\n",
      "No obvious bad files found.\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, glob\n",
    "from pathlib import Path\n",
    "\n",
    "d = Path(\"../data/training_data/vit\")\n",
    "files = sorted(d.glob(\"*.pt\"))\n",
    "print(\"Found files:\", len(files))\n",
    "\n",
    "bad = []\n",
    "stats = {\"img_min\":[], \"img_max\":[], \"img_mean\":[], \"kp_min\":[], \"kp_max\":[], \"kp_len\":[]}\n",
    "\n",
    "for p in files:\n",
    "    data = torch.load(p)\n",
    "    img = data[\"image\"]\n",
    "    kps = data[\"keypoints\"]\n",
    "    # image checks\n",
    "    stats[\"img_min\"].append(float(img.min()))\n",
    "    stats[\"img_max\"].append(float(img.max()))\n",
    "    stats[\"img_mean\"].append(float(img.mean()))\n",
    "    # keypoint checks\n",
    "    stats[\"kp_min\"].append(float(kps.min()))\n",
    "    stats[\"kp_max\"].append(float(kps.max()))\n",
    "    stats[\"kp_len\"].append(int(kps.numel()))\n",
    "    # sanity\n",
    "    if kps.shape not in [(9,2),(18,)]:\n",
    "        bad.append((p, \"bad shape\", kps.shape))\n",
    "    if len(kps) == 0:\n",
    "        bad.append((p, \"zero keypoints\"))\n",
    "    # any kp outside 0..224?\n",
    "    if (kps < -10).any() or (kps > 1000).any():\n",
    "        bad.append((p, \"kps extreme values\"))\n",
    "        \n",
    "print(\"Image min/max/mean (examples):\", np.percentile(stats[\"img_min\"], [0,50,100]), np.percentile(stats[\"img_max\"], [0,50,100]), np.mean(stats[\"img_mean\"]))\n",
    "print(\"Kp min/max examples:\", np.percentile(stats[\"kp_min\"], [0,50,100]), np.percentile(stats[\"kp_max\"], [0,50,100]))\n",
    "print(\"Keypoint tensor sizes (numel): unique:\", set(stats[\"kp_len\"]))\n",
    "if bad:\n",
    "    print(\"Bad files (first 10):\", bad[:10])\n",
    "else:\n",
    "    print(\"No obvious bad files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5bc36-9ed9-4a7a-8ce5-8d17dd7620ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lizard",
   "language": "python",
   "name": "lizard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
